# Ansible Playbook for Jupyter notebook

This Ansible Playbook manages Jupyter notebook behind Nginx proxy.

The playbook supposes to have two instance groups in inventory; *notebook* and *webproxy*.
Instances in *notebook* group run Jupyter notebook on conda environment.
Instances in *webproxy* group run Nginx to listen 80 port and to proxy HTTP requests to upstream notebook servers.

## Getting started

At first, you have to prepare Ansible control machine along with the [installation documentation](http://docs.ansible.com/ansible/latest/intro_installation.html),
and make sure it communicate with managed nodes using ssh.
Managed nodes are to be installed Python 2.6 or later.
If path to Python in managed nodes is different from standard location, you will set *ansible_python_interpreter* variable on your inventory file.

Since this playbook depends on several Galaxy modules written in `requirements.yml`, you begin with `ansible-galaxy` command on cotrol machine.

```bash
$ ansible-galaxy install -r requirements.yml
```

And, you prepare inventory file as follows.
You can see samples under *environments/* in this repository.

```ini
[notebook]
notebook1

[webproxy]
webproxy1

[vars:all]
ansible_python_interpreter=/usr/bin/python3
```

Then, run playbook for your inventory.

```bash
$ ansible-playbook -i your-inventory site.yml
```

Once you open web browser and access to web proxy URL, you are asked to input password for login.
Default password is *JupyterNotebook-t0pSecretCredential*,
and hash token is written in `var/all.yml` generated by *notebook.auth.passwd* function.
You can generate your own hash token on your first notebook as follows.

```python
from notebook.auth import passwd
passwd()
```

You should copy and paste hashed string to *jupyternb_password* in `var/all.yml`,
and run `notebook.yml` playbook again to update notebook configuration.

Default configuration uses [Google Public NTP](https://developers.google.com/time/) to handle leap seconds smoothly.
If you would like to other NTP servers, change *ntp_servers* variable in `var/all.yml`.

## Developing

Developers prepare three instances using Vagrant; *notebook*, *webproxy*, and *controller*.
*controller* instance is a Ansible control machine to run playbook and Serverspec test suite against *notebook* and *webproxy* instances.

### Built with

Following hands on experiences are required:

- Vagrant
- Ansible and Galaxy
- Serverspec

### Prerequisites

Install the latest version of Vagrant using official installer and make sure `vagrant` in the *PATH*.

```bash
$ vagrant --version
Vagrant 2.0.0
```

### Testing

You spin up the instances and run Serverspec suite on control machine after applying playbook.
Thanks to Ansible local provisioner, `vagrant up` command downloads Galaxy module and runs playbook automatically.

```bash
$ vagrant up
```

To test installation for *notebook* and *webproxy*, you login to controller instance and run Serverspec test suite.

```bash
$ vagrant ssh controller

(controller) $ cd /vagrant
(controller) $ rake spec
```

`controller.yml` playbook installs rvm and Serverspec on control machine,
and the playbook is hooked on ansible provisioner by `localdev.yml`.

## Installed software in instances

*notebook*

- ntp
- miniconda
- jupyter based on anaconda

*webproxy*

- ntp
- nginx

*controller*

- ntp
- rvm
- serverspec

